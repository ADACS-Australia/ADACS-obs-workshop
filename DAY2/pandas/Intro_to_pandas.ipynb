{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADACS introductory data analysis workshop\n",
    "\n",
    "This lesson is adapted from the [Data Carpentry Ecology lesson](http://www.datacarpentry.org/python-ecology-lesson/)\n",
    "- make sure to open this ipython notebook in the same directory as the data used in this notebook\n",
    "\n",
    "We'll be using an etherpad to share solutions to challenges, ask questions and chat:\n",
    "- etherpad let's you collaborate simultaniously on the same document\n",
    "- everyone has their own identificating colour\n",
    "- there is also a chat function where you can talk and ask questions if you like\n",
    "Your isntructor will give you the link to the etherpad on the day of the course.\n",
    "After finishing the course and updated notebook with the answers will be made available in the github repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap: quick intro to Python\n",
    "\n",
    "Python is a high-level, interpreted programming language. This means the code is easy to read for humans and there is no need for us to compile it and in many cases we do not have to think too much about the underlying system fro e.g. memory usage.\n",
    "\n",
    "As a consequence, we can use it in two ways:\n",
    "- Using the interpreter as an \"advanced calculator\" in interactive mode:\n",
    "- Executing programs/scripts saved as a text file, usually with *.py extension:\n",
    "\n",
    "\n",
    "## Data types\n",
    "How information is stored in a DataFrame or a python object affects what we can do with it and the outputs of calculations as well. There are two main types of data that we're explore in this lesson: numeric and character types.\n",
    "\n",
    "\n",
    "**Numeric Data Types**\n",
    "\n",
    "- integer\n",
    "- float\n",
    "\n",
    "**Character Data Types**\n",
    "\n",
    "- strings (a word, a sentence, or several sentences)\n",
    "- strings that contain numbers can not be used for mathematical operations!\n",
    "\n",
    "**Lists** \n",
    "\n",
    "are a common data structure to hold an ordered sequence of\n",
    "elements. Each element can be accessed by an index.  Note that Python\n",
    "indexes start with 0 instead of 1:\n",
    "\n",
    "**Tuple**  \n",
    "\n",
    "Similar to a list in that it's an ordered sequence of elements. However,\n",
    "tuples can not be changed once created (they are \"immutable\"). Tuples are\n",
    "created by placing comma-separated values inside parentheses `()`.\n",
    "\n",
    "**Dictionary** \n",
    "\n",
    "A container that holds pairs of objects - keys and values.\n",
    "\n",
    "Dictionaries work a lot like lists - except that you index them with *keys*. \n",
    "You can think about a key as a name for or a unique identifier for a set of values\n",
    "in the dictionary. Keys can only have particular types - they have to be \n",
    "\"hashable\". Strings and numeric types are acceptable, but lists aren't.\n",
    "\n",
    "\n",
    "## Operators\n",
    "We can perform mathematical calculations in Python using the basic operators\n",
    " `+, -, /, *, %`:\n",
    " \n",
    "** In python 2 if we divide one integer by another, we get an integer! **\n",
    "The result in python 3 is different where we get a float.\n",
    "Remember to convert your integers to floats when you want floating point precision for divisions!\n",
    "\n",
    "\n",
    "We can also use comparison and logic operators:\n",
    "`<, >, ==, !=, <=, >=` and statements of identity such as\n",
    "`and, or, not`. The data type returned by this is \n",
    "called a _boolean_.\n",
    " \n",
    "## Scripting\n",
    "\n",
    " **Comments** start with #\n",
    " \n",
    " **Methods** are a way to interact with an object (a list, for example). We can invoke \n",
    "a method using the dot `.` followed by the method name and a list of arguments in parentheses. \n",
    "To find out what methods are available for an object, we can use the built-in `help` command:\n",
    "\n",
    "\n",
    "A **Library** in Python contains a set of tools (called functions) that perform\n",
    "tasks on our data. \n",
    "\n",
    "Python doesn't load all of the libraries available to it by default. We have to\n",
    "add an `import` statement to our code in order to use library functions. To import\n",
    "a library, we use the syntax `import libraryName`. If we want to give the\n",
    "library a nickname to shorten the command, we can add `as nickNameHere`. \n",
    "\n",
    "You only need to load a library once during your session. You can load the library when needed\n",
    "or you can load all necessary libraries at the beginning of your script. \n",
    "This is good practice, especially for the readability of your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working With Pandas DataFrames in Python\n",
    "\n",
    "\n",
    "One of the best options for working with tabular data in Python is to use the\n",
    "[Python Data Analysis Library](http://pandas.pydata.org/) (a.k.a. Pandas). The\n",
    "Pandas library provides data structures, produces high quality plots with\n",
    "[matplotlib](http://matplotlib.org/) and integrates nicely with other libraries\n",
    "that use [NumPy](http://www.numpy.org/) (which is another Python library) arrays.\n",
    "\n",
    "Each time we call a function that's in a library, we use the syntax\n",
    "`LibraryName.FunctionName`. Adding the library name with a `.` before the\n",
    "function name tells Python where to find the function. In the example above, we\n",
    "have imported Pandas as `pd`. This means we don't have to type out `pandas` each\n",
    "time we call a Pandas function.\n",
    "\n",
    "\n",
    "## Starting in the same spot\n",
    "\n",
    "To help the lesson run smoothly, let's ensure everyone is in the same directory.\n",
    "This should help us avoid path and file name issues. At this time please\n",
    "navigate to the workshop directory. If you working in IPython Notebook be sure\n",
    "that you start your notebook in the workshop directory.\n",
    "\n",
    "A quick aside that there are Python libraries like [OS\n",
    "Library](https://docs.python.org/3/library/os.html) that can work with our\n",
    "directory structure, however, that is not our focus today.\n",
    "\n",
    "If you need to change your directory ```import os``` and use ```os.chdir```\n",
    "\n",
    "Or you can use **%** to access the command line, e.g. ```% cd folder_name```\n",
    "\n",
    "\n",
    "## Our Data \n",
    "\n",
    "For this lesson, we will be using [Galaxy Zoo DR1 data](https://www.google.com/search?q=galaxy+zoo&ie=utf-8&oe=utf-8&client=firefox-b-ab). Galaxy Zoo is described in Lintott et al. 2008, MNRAS, 389, 1179 and the data release is described in Lintott et al. 2011, 410, 166.\n",
    "\n",
    "The table we use is an adapted version of Table 2, listing classifications of galaxies which have spectra included in SDSS Data Release 7. The debiased fraction of the votes in elliptical and spiral categories is given, along with flags identifying systems as classified as spiral, elliptical or uncertain.\n",
    "\n",
    "\n",
    "| Column           | Description                             |\n",
    "|------------------|-----------------------------------------|\n",
    "| id               | SDSS ID, objects taken from DR7         |\n",
    "| ra               | Right Ascension  (HMS)                  |\n",
    "| dec              | Declination (DMS)                       |\n",
    "| nvote            | number of votes                         |\n",
    "| p_e              | debiased vote fraction Ellipticals      |\n",
    "| p_s              | debiased vote fraction all Spirals      |\n",
    "| type             | whether final vote is E, S or U         |\n",
    "| class            | spiral or elliptical class, eg E0 or CW |\n",
    "\n",
    "Galaxies flagged as ‘elliptical’ or ‘spiral’ require 80 per cent of the vote in that category after the debiasing procedure has been applied; all other galaxies are flagged ‘uncertain’.\n",
    "Note, the elliptical class is randomly assigned. The spiral class is based on the highest vote fraction of the spiral classes in the Galaxy Zoo DR 1 Table 2 data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#if you need to change your directory\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"data/\") #make sure you enter the correct fille path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#check your version, we need v0.19 or higher\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing With Data In Pandas\n",
    "\n",
    "We will begin by locating and reading our survey data which are in CSV format.\n",
    "We can use Pandas' `read_csv` function to pull the file directly into a\n",
    "[DataFrame](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe).\n",
    "\n",
    "## So What's a DataFrame?\n",
    "\n",
    "A DataFrame is a 2-dimensional data structure that can store data of different\n",
    "types (including characters, integers, floating point values, factors and more)\n",
    "in columns. It is similar to a spreadsheet or an SQL table or the `data.frame` in\n",
    "R. A DataFrame always has an index (0-based). An index refers to the position of \n",
    "an element in the data structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# note that pd.read_csv is used because we imported pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Ways to View DataFrame objects in Python\n",
    "\n",
    "\n",
    "First, let's check the data type of the data stored in `gal_df` using the `type` method. \n",
    "The `type` method and `__class__` attribute tell us that `gal_df` is `<class 'pandas.core.frame.DataFrame'>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# or gal_df.__class__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also enter `gal_df.dtypes` in our prompt to view the data type for each\n",
    "column in our DataFrame. `int64` represents numeric integer values - `int64` cells\n",
    "can not store decimals. `object` represents strings (letters and numbers). `float64`\n",
    "represents numbers with decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas and base Python use slightly different names for data types. More on this\n",
    "is in the table below:\n",
    "\n",
    "| Pandas Type | Native Python Type | Description |\n",
    "|-------------|--------------------|-------------|\n",
    "| object | string | The most general dtype. Will be assigned to your column if column has mixed types (numbers and strings). |\n",
    "| int64  | int | Numeric characters. 64 refers to the memory allocated to hold this character. |\n",
    "| float64 | float | Numeric characters with decimals. If a column contains numbers and NaNs(see below), pandas will default to float64, in case your missing value has a decimal. |\n",
    "| datetime64, timedelta[ns] | N/A (but see the [datetime] module in Python's standard library) | Values meant to hold time data. Look into these for time series experiments. |\n",
    "\n",
    "[datetime]: http://doc.python.org/2/library/datetime.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple other methods that can be used to summarize and access the data\n",
    "stored in DataFrames. Let's try out a few. Note that we call the method by using\n",
    "the object name `gal_df.method`. So `gal_df.columns` provides an index\n",
    "of all of the column names in our DataFrame.\n",
    "\n",
    "### Challenges\n",
    "\n",
    "Try out the methods below to see what they return.\n",
    "\n",
    "1. `gal_df.columns`.\n",
    "2. `gal_df.head()`. Also, what does `gal_df.head(15)` do?\n",
    "3. `gal_df.tail()`.\n",
    "4. `gal_df.shape`. Take note of the output of the shape method. What format does it return the shape of the DataFrame in?\n",
    "\n",
    "HINT: [More on tuples, here](https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there were 667944 rows and 8 columns parsed.\n",
    "It looks like  the `read_csv` function in Pandas  read our file properly.\n",
    "\n",
    "Now we can start manipulating our data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data Using Labels (Column Headings)\n",
    "\n",
    "We use square brackets `[]` to select a subset of an Python object. For example,\n",
    "we can select all of the data from a column named `type` from the `gal_df`\n",
    "DataFrame by name:\n",
    "\n",
    "```python\n",
    "gal_df['type']\n",
    "# this syntax, calling the column as an attribute, gives you the same output\n",
    "gal_df.type\n",
    "```\n",
    "\n",
    "We can also create an new object that contains the data within the `type`\n",
    "column as follows:\n",
    "\n",
    "```python\n",
    "# create an object named gal_types that only contains the `types` column\n",
    "gal_types = gal_df['types']\n",
    "```\n",
    "\n",
    "We can pass a list of column names too, as an index to select columns in that\n",
    "order. This is useful when we need to reorganize our data.\n",
    "\n",
    "**NOTE:** If a column name is not contained in the DataFrame, an exception\n",
    "(error) will be raised.\n",
    "\n",
    "```python\n",
    "# select the type and class columns from the DataFrame\n",
    "gal_df[['type', 'class']]\n",
    "# what happens when you flip the order?\n",
    "gal_df[['class', 'type']]\n",
    "#what happens if you ask for a column that doesn't exist?\n",
    "gal_df['types']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick aside: Python and integer division\n",
    "\n",
    "**In python 2 integer division returns and integer, as opposed to python 3 where we get a float.**\n",
    "If at least one of the numebrs is a float then we get a float.\n",
    "Alternatively we could use ```from __future__ import division```, which will then treat integer division in\n",
    "python 2 as python 3. However, be aware that if the way python handles division is changed in the future your code might break. So choose what you think is the most stable option for your scripts!\n",
    "\n",
    "\n",
    "To modify the format of values within our data frame we can use the ```astype``` function (also remember in pandas the type is `float64`).\n",
    "Don't forget this is a function within pandas so we use it with a `.`, for example to convert \n",
    "the `nvote` field to floating point values we would run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert the nvote field from an integer to a float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remember to use from __future__ import if you are using Python 2\n",
    "from __future__ import division\n",
    "10/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we try to convert probability values to integers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this throws a value error: `ValueError: Cannot convert NA to\n",
    "integer`. If we look at the `weight` column in the surveys data we notice that\n",
    "there are NaN (**N**ot **a** **N**umber) values. *NaN* values are undefined\n",
    "values that cannot be represented mathematically. Pandas, for example, will read\n",
    "an empty cell in a CSV or Excel sheet as a NaN. NaNs have some desirable\n",
    "properties: if we were to average the `weight` column without replacing our NaNs,\n",
    "Python would know to skip over those cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: older pandas version do not know how to handle NaN, please update to v0.19_\n",
    "\n",
    "Check your pandas version using `pd.__version__`, if you need to update open a bash shell\n",
    "and type ```conda update pandas```.\n",
    "\n",
    "---\n",
    "\n",
    "## Missing Data Values - NaN\n",
    "\n",
    "Dealing with missing data values is always a challenge. It's sometimes hard to\n",
    "know why values are missing - was it because of a data entry error? Or data that\n",
    "someone was unable to collect? Should the value be 0? We need to know how\n",
    "missing values are represented in the dataset in order to make good decisions.\n",
    "If we're lucky, we have some metadata that will tell us more about how null\n",
    "values were handled.\n",
    "\n",
    "For instance, in some disciplines, like Remote Sensing, missing data values are\n",
    "often defined as -9999. Having a bunch of -9999 values in your data could really\n",
    "alter numeric calculations. Often in spreadsheets, cells are left empty where no\n",
    "data are available. Pandas will, by default, replace those missing values with\n",
    "NaN. However it is good practice to get in the habit of intentionally marking\n",
    "cells that have no data, with a no data value! That way there are no questions\n",
    "in the future when you (or someone else) explores your data.\n",
    "\n",
    "### Where Are the NaN's?\n",
    "\n",
    "Let's explore the NaN values in our data a bit further. \n",
    "First, let's figure out how many rows contain NaN values for weight. \n",
    "We can do this by identifying how many rows have a NULL value (`.isnull`) or by counting the number of rows that have a meaningful value (e.g., p_e>0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replace all NaN values with zeroes using the `.fillna()` method (after\n",
    "making a copy of the data so we don't lose our work).\n",
    "\n",
    "However, NaN and 0 yield different analysis results. The mean value when NaN\n",
    "values are replaced with 0 is different from when NaN values are simply thrown\n",
    "out or ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replace nan with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check mean, how does it differ from before?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fill NaN values with any value that we chose. The code below fills all\n",
    "NaN values with a mean for all probabilityvalues.\n",
    "\n",
    "```python\n",
    " df1['p_e'] = gal_df['p_e'].fillna(gal_df['p_e'].mean())\n",
    "```\n",
    "\n",
    "We could also chose to create a subset of our data, only keeping rows that do\n",
    "not contain NaN values, using `.dropna()` method.\n",
    "\n",
    "**The point is to make conscious decisions about how to manage missing data.** \n",
    "This is where we think about how our data will be used and how these values will\n",
    "impact the scientific conclusions made from the data.\n",
    "\n",
    "Python gives us all of the tools that we need to account for these issues. We\n",
    "just need to be cautious about how the decisions that we make impact scientific\n",
    "results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Statistics From Data In A Pandas DataFrame\n",
    "\n",
    "We've read our data into Python. Next, let's perform some quick summary\n",
    "statistics to learn more about the data that we're working with. We might want\n",
    "to know how many galaxies of a specific type we have, or what the average number of votes was. \n",
    "We can perform summary stats quickly using groups. But first we need to figure out what we want to group by.\n",
    "\n",
    "Let's begin by exploring our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gal_df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a list of all the galaxy types. The `pd.unique` function tells us all of\n",
    "the unique values in the `type` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "1. What is the difference between `len(gal_df.nvote)` and `gal_df['nvote'].nunique()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "The Pandas function `describe` will return descriptive stats including: mean,\n",
    "median, max, min, std and count for a particular column in the data. Pandas'\n",
    "`describe` function will only return summary values for columns containing\n",
    "numeric data.\n",
    "We can calculate basic statistics for all records in a single column using the\n",
    "syntax below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gal_df['nvote'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract one specific metric if we wish:\n",
    "\n",
    "```python\n",
    "gal_df['nvote'].min()\n",
    "gal_df['nvote'].max()\n",
    "gal_df['nvote'].mean()\n",
    "gal_df['nvote'].std()\n",
    "gal_df['nvote'].count()\n",
    "```\n",
    "\n",
    "## Groups in Pandas and summary stats\n",
    "\n",
    "But if we want to summarize by one or more variables, for example galaxy type, we can\n",
    "use Pandas' `.groupby` method. Once we've created a groupby DataFrame, we\n",
    "can quickly calculate summary statistics by a group of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summary statistics for all numeric columns by type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# provide the mean for each numeric column by type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `groupby` command is powerful in that it allows us to quickly generate\n",
    "summary stats.\n",
    "\n",
    "\n",
    "Let's next count the number of galaxies for each type. We can do this in a few\n",
    "ways, but we'll use `groupby` combined with a `count()` method.\n",
    "\n",
    "\n",
    "```python\n",
    "# count the number of samples by type\n",
    "type_counts = gal_df.groupby('type')['id'].count()\n",
    "```\n",
    "\n",
    "Or, we can also count just the rows that have type='U':\n",
    "\n",
    "```python\n",
    "gal_df.groupby('type')['id'].count()['U']\n",
    "```\n",
    "\n",
    "\n",
    "### Challenges\n",
    "\n",
    "1. What happens when you group by two columns using the following syntax and\n",
    "    then grab mean values:\n",
    "\t- `sorted_data2 = gal_df.groupby(['type','p_e'])`\n",
    "\t- `sorted_data2.mean()`\n",
    "2. Summarize number of votes for each galaxy class in your data. HINT: you can use the\n",
    "   following syntax to only create summary statistics for one column in your data\n",
    "   `by_class['nvote'].describe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Basic Math Functions\n",
    "\n",
    "If we wanted to, we could perform math on an entire column of our data. For\n",
    "example let's multiply all votes by 2. A more practical use of this might\n",
    "be to normalize the data according to a mean, area, or some other value\n",
    "calculated from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(gal_df.nvote.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# multiply all votes by 2\n",
    "gal_df['nvote']*=2\n",
    "gal_df.nvote.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick & Easy Plotting Data Using Pandas\n",
    "\n",
    "We can plot our summary stats using Pandas, too. Check the [documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make sure figures appear inline in Jupyter Notebook\n",
    "%matplotlib inline\n",
    "# create a quick bar chart of the number of votes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#alternatively use the .hist plotting option\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We can also look at how many galaxies were assigned to each class and plot it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "\n",
    "1. Create a plot of average votes across all class of galaxies.\n",
    "2. Create a plot of total votes for each type of galaxy for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing & Slicing in Python\n",
    "\n",
    "We often want to work with subsets of a **DataFrame** object. There are\n",
    "different ways to accomplish this including: using labels (ie, column headings - as used previously),\n",
    "numeric ranges or specific x,y index locations.\n",
    "\n",
    "\n",
    "**REMINDER**: Python Uses 0-based Indexing\n",
    "\n",
    "\n",
    "[indexing diagram](https://github.com/datacarpentry/python-ecology-lesson/blob/gh-pages/fig/slicing-indexing.svg)\n",
    "\n",
    "[slicing diagram](https://github.com/datacarpentry/python-ecology-lesson/tree/gh-pages/fig/slicing-slicing.svg)\n",
    "\n",
    "\n",
    "## Slicing Subsets of Rows in Python\n",
    "\n",
    "Slicing using the `[]` operator selects a set of rows and/or columns from a\n",
    "DataFrame. To slice out a set of rows, you use the following syntax:\n",
    "`data[start:stop]`. When slicing in pandas the start bound is included in the\n",
    "output. The stop bound is one step BEYOND the row you want to select. So if you\n",
    "want to select rows 0, 1 and 2 your code would look like this:\n",
    "\n",
    "```python\n",
    "# select rows 0,1,2 (but not 3)\n",
    "gal_df[0:3]\n",
    "```\n",
    "\n",
    "The stop bound in Python is different from what you might be used to in\n",
    "languages like Matlab and R.\n",
    "\n",
    "```python\n",
    "# select the first, second and third rows from the surveys variable\n",
    "gal_df[0:3]\n",
    "# select the first 5 rows (rows 0,1,2,3,4)\n",
    "gal_df[:5]\n",
    "# select the last element in the list\n",
    "gal_df[-1:]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gal_df[0:5:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also reassign values within subsets of our DataFrame. But before we do that, let's make a \n",
    "copy of our DataFrame so as not to modify our original imported data. \n",
    "\n",
    "```python\n",
    "# copy the surveys dataframe so we don't modify the original DataFrame\n",
    "gal_copy = gal_df\n",
    "\n",
    "# set the first three rows of data in the DataFrame to 0\n",
    "gal_copy[0:3] = 0\n",
    "```\n",
    "\n",
    "Next, try the following code: \n",
    "\n",
    "```python\n",
    "gal_copy.head()\n",
    "gal_df.head()\n",
    "```\n",
    "What is the difference between the two data frames?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencing Objects vs Copying Objects in Python\n",
    "\n",
    "We might have thought that we were creating a fresh copy of the `gal_df` objects when we \n",
    "used the code `surveys_copy = gal_df`. However the statement  y = x doesn’t create a copy of our DataFrame. \n",
    "It creates a new variable y that refers to the **same** object x refers to. This means that there is only one object \n",
    "(the DataFrame), and both x and y refer to it. So when we assign the first 3 columns the value of 0 using the \n",
    "`surveys_copy` DataFrame, the `gal_df` DataFrame is modified too. To create a fresh copy of the `gal_df`\n",
    "DataFrame we use the syntax y=x.copy(). But before we have to read the gal_df again because the current version contains the unintentional changes made to the first 3 columns.\n",
    "\n",
    "```python\n",
    "gal_df = pd.read_csv(\"GalaxyZoo1.csv\")\n",
    "gal_copy= gal_df.copy()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read data back in and check it's correct\n",
    "gal_df = pd.read_csv(\"GalaxyZoo1.csv\")\n",
    "gal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#copy data frame and check the copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#modify copy and check both copy and original to see changes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing Subsets of Rows and Columns in Python\n",
    "\n",
    "We can select specific ranges of our data in both the row and column directions\n",
    "using either label or integer-based indexing.\n",
    "\n",
    "- `loc`: indexing via *labels* (which can be numbers)\n",
    "- `iloc`: indexing via *integers*\n",
    "\n",
    "To select an index subset of rows AND columns from our DataFrame, we can use the `iloc`\n",
    "method. For example, we can select RA, Dec and number of votes (columns 2, 3 and 4 if we\n",
    "start counting at 1), like this:\n",
    "\n",
    "```python\n",
    "gal_df.iloc[0:3, 1:4]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we asked for a slice from 0:3. This yielded 3 rows of data. When you\n",
    "ask for 0:3, you are actually telling python to start at index 0 and select rows\n",
    "0, 1, 2 **up to but not including 3**.\n",
    "\n",
    "\n",
    "Next let's explore some other ways to index and select subsets of data using `loc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# what does this do?\n",
    "gal_df.loc[0, ['id', 'p_e', 'p_s']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What happens when you run the code below? Try using iloc instead of loc\n",
    "gal_df.loc[[0, 10, 668946], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gal_df.iloc[[0, 10, 668946], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: If using `iloc` labels must be found in the DataFrame or you will get a `KeyError`. Using `loc` (at least in newer versions of Pandas) you'll get NaN entries returned so be careful!\n",
    "\n",
    "The start bound and the stop bound are **included**.  When using `loc`, integers\n",
    "*can* also be used, but they refer to the **index label** and not the position. Thus\n",
    "when you use `loc`, and select 1:4, you will get a different result than using\n",
    "`iloc` to select rows 1:4.\n",
    "\n",
    "We can also select a specific data value according to the specific row and\n",
    "column location within the data frame using the `iloc` function:\n",
    "`dat.iloc[row,column]`.\n",
    "\n",
    "\n",
    "```python\n",
    "gal_df.iloc[2,6]\n",
    "```\n",
    "\n",
    "Remember that Python indexing begins at 0. So, the index location [2, 6] selects\n",
    "the element that is 3 rows down and 7 columns over in the DataFrame.\n",
    "\n",
    "## Challenges\n",
    "\n",
    "1. What happens when you type:\n",
    "\t- `gal_df[0:4]`\n",
    "\t- `gal_df[:5]`\n",
    "\t- `gal_df[-1:]`\n",
    "\n",
    "2. What happens when you call:\n",
    "    - `gal_df.iloc[0:4, 1:4]`\n",
    "    - `gal_df.loc[0:4, ['ra','dec','nvote']]`\n",
    "    - How are the two commands different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting Data Using Criteria & Making Masks\n",
    "\n",
    "A mask can be useful to locate where a particular subset of values exist or\n",
    "don't exist - for example,  NaN, or \"Not a Number\" values. To understand masks,\n",
    "we also need to understand `BOOLEAN` objects in python.\n",
    "\n",
    "Boolean values include `true` or `false`. So for example\n",
    "\n",
    "```python\n",
    "# set x to 5\n",
    "x = 5\n",
    "# what does the code below return?\n",
    "x > 5\n",
    "# how about this?\n",
    "x == 5\n",
    "```\n",
    "When we ask python what the value of `x > 5` is, we get `False`. This is because x\n",
    "is not greater than 5 it is equal to 5. To create a boolean mask, you first create the\n",
    "True / False criteria (e.g. values > 5 = True). Python will then assess each\n",
    "value in the object to determine whether the value meets the criteria (True) or\n",
    "not (False). Python creates an output object that is the same shape as\n",
    "the original object, but with a True or False value for each index location.\n",
    "\n",
    "You can use the syntax below when querying data from a DataFrame. Experiment\n",
    "with selecting various subsets of our data.\n",
    "\n",
    "* Equals: `==`\n",
    "* Not equals: `!=`\n",
    "* Greater than, less than: `>` or `<`\n",
    "* Greater than or equal to `>=`\n",
    "* Less than or equal to `<=`\n",
    "\n",
    "Let's try this out. \n",
    "We can select all rows that have 80 votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Or we can select all rows that do not contain 80 votes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We can define sets of criteria too:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's identify all locations in the survey data that have\n",
    "null (missing or NaN) data values. We can use the `isnull` method to do this.\n",
    "Each cell with a null value will be assigned a value of  `True` in the new\n",
    "boolean object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the rows where there are null values,  we can use \n",
    "the mask as an index to subset our data as follows:\n",
    "\n",
    "```python\n",
    "#To select just the rows with NaN values, we can use the .any method\n",
    "gal_df[pd.isnull(gal_df).any(axis=1)]\n",
    "```\n",
    "\n",
    "We can run isnull on a particular column too. What does the code below do?\n",
    "\n",
    "```python\n",
    "# what does this do?\n",
    "empty_pE = gal_df[pd.isnull(gal_df['p_e'])\n",
    "```\n",
    "\n",
    "Let's take a minute to look at the statement above. \n",
    "\n",
    "We are using the Boolean object as an index. \n",
    "We are asking python to select rows that have a `NaN` value for the probability of a galaxy being Elliptical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "\n",
    "1. You can use the `isin` command in python to query a DataFrame based upon a list of values as follows:\n",
    "   `gal_df[gal_df['class'].isin([listGoesHere])]`. Use the `isin` function to find all Elliptical galaxies of class E0, E1 and E7. How many records contain these values?\n",
    "2. The `~` symbol in Python can be used to return the OPPOSITE of the selection that you specify in python. It is equivalent to **is not in**. Write a query that selects all Spiral galaxies that are not classed as 'OTHER' an 'EDGE'.\n",
    "3. Create a new DataFrame that only contains valid (i.e. no NAN probability) observations of Spiral galaxies with over 100 votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# challenge 1 -> count of E0,E1,E7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#challenge 2 -> all spirals that are not OTHER or EDGE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#challenge 3 -> spiral probaility is column 'p_s'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepping our data for machine learning\n",
    "\n",
    "## Problem definition\n",
    "\n",
    "We have been tasked to develop a machine learning model to classify galaxies by their morphology (appearance) from a dataset containing measurements such as:\n",
    "\n",
    "- redshift\n",
    "- magnitude in _ugriz_ bands\n",
    "- exponential scale radius and ellipticity\n",
    "- de Vaucouleurs scale radius and ellipticity\n",
    "- stellar mass\n",
    "- ...\n",
    "- etc.\n",
    "\n",
    "Galaxies in the dataset have labels taken from [Galaxy Zoo](https://www.galaxyzoo.org/) DR1 - Table 2. Galaxy Zoo is described in Lintott et al. 2008, MNRAS, 389, 1179 and the data release is described in Lintott et al. 2011, 410, 166. \n",
    "\n",
    "We use the final debiased labels to categorise a galaxy as:\n",
    "\n",
    "- spiral\n",
    "- elliptical\n",
    "\n",
    "![Galaxies](http://4.bp.blogspot.com/_rfhv4lPQhSY/TUQjfMLtrxI/AAAAAAAABAo/W3OF0mXZNZc/s1600/spiral%2Bor%2Belliptical%2Bzoo%2B1.jpg)\n",
    "\n",
    "<div style=\"text-align:center;font-weight:bold\">Figure: A spiral galaxy (left) and elliptical galaxy (right)</div>\n",
    "\n",
    "Our goal is to train a model that can accurately classify galaxies. We want our model to generalise well. i.e. it can correctly classify unseen galaxies (i.e. galaxies not in our training dataset).\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "For our introduction to machine learning, we will be using a subset of the data we worked with so far. To be able to try several machine learning algorithms we will be using a data set that combines [Galaxy Zoo DR1](https://www.galaxyzoo.org/) and the Sloan Digital Sky Survey (SDSS) ([using the DR9 SQL search](http://skyserver.sdss.org/dr9/en/tools/search/sql.asp)). \n",
    "\n",
    "The data dictionary for this dataset is presented in Table 1. This dataset is limited to the first 5,000 Galaxy Zoo classified galaxies which have spectra in the SDSS database. The debiased fraction of the votes in elliptical and spiral categories is given, along with columns identifying systems classified as spiral, elliptical or uncertain.\n",
    "\n",
    "<p style=\"text-align:center;font-weight:bold\">Table 1: Data dictionary</p>\n",
    "\n",
    "| Column           | Description                                                            |\n",
    "|:-----------------|:-----------------------------------------------------------------------|\n",
    "| id               | unique SDSS ID composed of [skyVersion, rerun, run, camcol, field, obj]|\n",
    "| ra               | right ascension  (HMS)                                                 |\n",
    "| dec              | declination (DMS)                                                      |\n",
    "| redshift         | redshift                                                               |\n",
    "| mag_u            | magnitude _u_ band                                                     |\n",
    "| mag_g            | magnitude _g_ band                                                     |\n",
    "| mag_r            | magnitude _r_ band                                                     |\n",
    "| mag_i            | magnitude _i_ band                                                     |\n",
    "| mag_z            | magnitude _z_ band                                                     |\n",
    "| deVRad_r         | de Vaucouleurs scale radius fit in _r_ band                            |\n",
    "| deVAB_r          | ellipticity from de Vaucouleurs fit in _r_ band                        |\n",
    "| expRad_r         | exponential scale radius fit in _r_ band                               |\n",
    "| expAB_r          | ellipticity from exponential fit in _r_ band                           |\n",
    "| stellar_mass     | log galaxy mass (in units of solar mass)                               |\n",
    "| votes            | number of Galaxy Zoo annotators                                        |\n",
    "| p_el_debiased    | debiased labelling probability the galaxy is elliptical                |\n",
    "| p_cs_debiased    | debiased labelling probability the galaxy is spiral                    |\n",
    "| spiral           | label = spiral galaxy {0=False, 1=True}                                |\n",
    "| elliptical       | label = elliptical galaxy {0=False, 1=True}                            |\n",
    "| uncertain        | label = uncertain {0=False, 1=True}                                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand and clean the data\n",
    "\n",
    "The next step is to explore and familiarise ourselves with the data using the methods we learnedabove. \n",
    "Datasets can have errors, and it's important to identify and resolve these errors before rushing in to develop our ML models. Errors in the dataset that aren't resolved will propogate through the machine learning pipeline and will bias our results.\n",
    "\n",
    "Generally, we're looking to answer the following questions:\n",
    "\n",
    "* Is there anything wrong with the data?\n",
    "* Are there any quirks with the data?\n",
    "* Do I need to clean or remove some of the data?\n",
    "\n",
    "Let's start by reading the data into a `pandas` DataFrame and inspecting the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the data file, make sure you are trying to access the correct folder/file\n",
    "data = pd.read_csv('galaxies.csv')\n",
    "\n",
    "# Display the first few rows using the .head() method\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data Values - NaN\n",
    "\n",
    "Dealing with missing data values is always a challenge, think about:\n",
    "* Why values are missing - was it because of a data entry error? Or are they data that someone was unable to collect? \n",
    "* How are the missing values represented? Are they empty cells in a spreadsheet, NaN or NA, or otherwise defined?\n",
    "* How should you treat the NaN? Should the value be 0 or the mean of all measurements, should they be ignored? \n",
    "\n",
    "Note: empty cells in a spreadhseet are automatically replaced with NaN by pandas when the data is read into a DataFrame.\n",
    "\n",
    " So, one of the first things we should do is to look for is missing data. We can use `pandas` to  display summary statistics for each (numerical) column as well as inspect rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The .describe() method returns summary statistics for each (numerical) column\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get records containing any missing values\n",
    "data[data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a number of rows that contain missing values (`NaN`). There are many [methods for handling and imputing missing data](http://machinelearningmastery.com/handle-missing-data-python/). However, from our results we see that the galaxies with missing values have been labelled as `uncertain` so we will simply remove these records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop the rows containing any missing values\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check to make sure the records have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Should return 0 rows\n",
    "data[data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the data with -9999 values, from our domain knowledge we know that this is a flag to signify NaN. So we can treat these as missing values / errors and remove them from our dataset as well.\n",
    "Also, having galaxies with masses or magnitudes less than 0 does not make sense and we should remove these records, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at and remove our data with values <0. \n",
    "\n",
    "Be careful, some data columns can be smaller than zero and still be valid measurements.\n",
    "\n",
    "Make sure to check any metadata given with your data to figure out what is a bad measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get records with ≤ 0 values using iloc\n",
    "# remember first number in slice is inclusive, second number is exclusive\n",
    "# Exclude the first  4 columns (id, ra, dec, redshift) and last 5 columns (galaxy labels and probabilities) \n",
    "data[(data.iloc[:, 4:-5] <= 0).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Only retain records with > 0 values in our dataset\n",
    "data = data[(data.iloc[:, 4:-5] > 0).all(axis=1)]\n",
    "\n",
    "# Let's check the summary stats again\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to construct a good training dataset for developing a robust model. In machine learning, we also refer to the training dataset as the ground truth. Any uncertainities or errors in the ground truth can confuse and reduce the performance of our models.\n",
    "\n",
    "In order to build a good training dataset, we only want to keep records where we are confident there is agreement between the Galaxy Zoo annotators in labelling galaxies. From the dataset, it appears we can achieved this by:\n",
    "\n",
    "- Keeping records where the uncertain label column is equal to False (0)\n",
    "\n",
    "Let's try this method and perform some sanity checking to see if the resulting data subset has debiased probabilities for ellipital and spiral labels ≥ 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Filter records using the uncertain column \n",
    "f1 = data[data['uncertain'] == 0]\n",
    "print('# records after removing uncertain rows: %d' % (len(f1)))\n",
    "\n",
    "# 2. Filter records using the debiased probabilities ≥ 0.5\n",
    "f2 = f1[(f1['p_el_debiased'] >= 0.5) | (f1['p_cs_debiased'] >= 0.5)]\n",
    "print('# records with ≥ 0.5 debiased probability: %d' % (len(f2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a discrepency of 5 records between the two data subsets. Let's investigate further by inspecting these records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display filter #1 rows that don't appear in the filter #2 dataset\n",
    "f1.loc[f1.index.difference(f2.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These records have been labelled as spiral galaxies but the debiased probabilities (confidence) for these labels are below the 0.5 confidence threshold. i.e. `p_cs_debiased < 0.5`. Additionally, for the last 2 records, we see that debiased probabilities for labelling the galaxy as elliptical is actually larger than the spiral probabilities which suggests these might be due to labelling errors. \n",
    "\n",
    "It appears we can't rely on filtering the dataset on the `uncertain` column alone so we will apply both filters to clean the dataset. Sanity checking your dataset is a step that is regularly overlooked but dedicating time to explore and resolve these errors can (sometimes dramatically) improve the performance of our machine learning models.\n",
    "\n",
    "We will make a copy of the filtered dataset in the variable `df` (abbreviation for DataFrame) for our clean dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use .copy to create and independent copy of the dataframe\n",
    "df = f2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean and prepare our dataset by:\n",
    "\n",
    "- creating a class column to represent the galaxy type label `{0=elliptical, 1=spiral}`\n",
    "- removing unwanted columns that won't be used in our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the class column - do this by renaming the 'spiral' column to 'class'\n",
    "df.rename(columns={'spiral': 'class'}, inplace=True)\n",
    "\n",
    "# Remove unwanted columns\n",
    "unwanted_columns = ['ra', 'dec', 'votes', 'p_el_debiased', \n",
    "                    'p_cs_debiased', 'elliptical', 'uncertain']\n",
    "df.drop(unwanted_columns, axis=1, inplace=True)\n",
    "\n",
    "# Inspect the first few rows of our prepared dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it's always a good idea to look to have a closer look at our data — especially for outliers. Let's start by printing out some summary statistics about the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display dataset summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary table provides some useful information:\n",
    "\n",
    "- there are 1,797 galaxies in our dataset\n",
    "- the mean of the `class` column indicates that ~75.3% of the dataset are spiral galaxies\n",
    "- min values for the radius columns range from 0.31 to 0.38\n",
    "- min value for the `stellar_mass` = 8.096\n",
    "- min value for the `mag` ranging from 15.59 to 18.5\n",
    "\n",
    "Our cleaned dataset contains 1,797 galaxies and 75.3% of the records are spiral galaxies.\n",
    "\n",
    "Tables like this are useful when we know that our data should fall in a particular range. e.g. It did not make sense to have zero or negative values for some of our columns. However, it is usually better to visualize the data in some way. Visualization makes outliers and errors immediately stand out, whereas they might go unnoticed in a large table of numbers.\n",
    "\n",
    "Since we know we're going to be plotting in this section, let's set up the notebook so we can plot inside of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Command to show plots inside of the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a **scatterplot matrix**. Scatterplot matrices plot the distribution of each column along the diagonal and a scatterplot matrix for the combination of each variable. They make for an efficient tool to look for errors in our data.\n",
    "\n",
    "Since we have quite a few columns, let's generate two scatterplots for data containing:\n",
    "\n",
    "1.  magnitudes\n",
    "2. measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Magnitudes scatterplot\n",
    "magnitudes = ['mag_u', 'mag_g', 'mag_r', 'mag_i', 'mag_z']\n",
    "selected_columns = magnitudes + ['class']\n",
    "sb.pairplot(df[selected_columns], hue='class', plot_kws={'alpha': 0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the magnitude scatterplot matrix, we see a potential outlier: the `mag_u` value of an `elliptical` galaxy falls outside its normal range (a value of ~24) and there seems to be an outlier for the spirals as well with values >18 for all(?) magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect possible outliers\n",
    "df[(df.loc[:, df.columns.str.startswith('mag')]>18).all(axis=1) | (df['mag_u'] > 24)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing obvious that leads us to believe the records are erroneous. Let's keep them in our dataset.\n",
    "\n",
    "Next we have a look at the distribution of the other measurements in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Measurements scatterplot\n",
    "measurements = ['redshift', 'stellar_mass', 'deVRad_r', 'deVAB_r', 'expRad_r', 'expAB_r']\n",
    "selected_columns = measurements + ['class']\n",
    "sb.pairplot(df[selected_columns], hue='class', plot_kws={'alpha': 0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatterplot matrix, we see potential outliers: \n",
    "\n",
    "- the `stellar_mass` value of a `spiral` galaxy falls outside its normal range (a value of ~8).\n",
    "- two spiral galaxies with `expRad_r` ≥ 17\n",
    "\n",
    "Let's inspect the outlier records to determine whether if we should keep them in our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect possible outliers\n",
    "df[(df['stellar_mass'] < 8.1) | (df['expRad_r'] >= 17)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing obvious that leads us to believe the records are erroneous. Let's keep them in our dataset.\n",
    "\n",
    "Fixing outliers can be difficult. It's often unclear whether the outlier was caused by measurement error, recording the data in improper units, or if the outlier is a real anomaly. For that reason, we should be careful when working with outliers. If we decide to exclude any data, we should document what data was excluded and provide reasons for excluding that data. i.e. The data didn't fit my hypothesis will not stand peer review.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Out Data to CSV\n",
    "\n",
    "After all this hard work, we don't want to repeat the data cleaning process every time we work with this dataset. Let's save the cleaned dataset as a separate file and work directly with that data file from now on.\n",
    "\n",
    "We can use the `to_csv` command to do export a DataFrame in CSV format. \n",
    "Note that by default the data will be saved into the current working directory. \n",
    "We can save it to a different folder by adding the foldername and a slash before the file name,\n",
    "either using relative or absolute path names.\n",
    "\n",
    "```python\n",
    "# Write DataFrame to CSV \n",
    "df.to_csv('galaxies-clean.csv')\n",
    "```\n",
    "\n",
    "Check out your working directory to make sure the CSV wrote out properly, and\n",
    "that you can open it! If you want, try to bring it back into python to make sure\n",
    "it imports properly.\n",
    "\n",
    "```python\t\n",
    "# for kicks read our output back into python and make sure all looks good\n",
    "new_output = pd.read_csv('galaxies-clean.csv', keep_default_na=False, na_values=[\"NA\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the clean dataset to a file (try saving it into the correct folder for tomorrows/DAY3 lesson)\n",
    "df.to_csv('data/galaxies-clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more useful info to get you started and keep you going with notebooks and pandas:\n",
    "\n",
    "**Notebook tips and tricks**\n",
    "https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/\n",
    "\n",
    "**Pandas cheat sheet**\n",
    "https://www.analyticsvidhya.com/blog/2015/07/11-steps-perform-data-analysis-pandas-python/\n",
    "\n",
    "\n",
    "**Join the ADACS facebook group to stay up-to-date with upcoming training!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging DataFrames\n",
    "\n",
    "\n",
    "In many \"real world\" situations, the data that we want to use come in multiple\n",
    "files. We often need to combine these files into a single DataFrame to analyze\n",
    "the data. The pandas package provides [various methods for combining\n",
    "DataFrames](http://pandas.pydata.org/pandas-docs/stable/merging.html) including\n",
    "`merge` and `concat`.\n",
    "\n",
    "To work through the examples below, we first need to load the galaxy zoo and\n",
    "SDSS files into pandas DataFrames. In iPython:\n",
    "\n",
    "``` python\n",
    "import pandas as pd\n",
    "gal_df = pd.read_csv('GalaxyZooSub.csv',\n",
    "                         keep_default_na=False, na_values=[\"NA\"])\n",
    "sdss = pd.read_csv('SDSS_blue_centre_query.csv',\n",
    "                         keep_default_na=False, na_values=[\"\"])\n",
    "gal_df.dtypes\n",
    "sdss.dtypes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gal_df = pd.read_csv('GalaxyZooSub.csv',\n",
    "                         keep_default_na=False, na_values=[\"NA\"])\n",
    "sdss = pd.read_csv('SDSS_blue_centre_query.csv',\n",
    "                         keep_default_na=False, na_values=[\"\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gal_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sdss.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note that the `read_csv` method we used can take some additional options which\n",
    "we didn't use previously. Many functions in python have a set of options that\n",
    "can be set by the user if needed. In this case, we have told Pandas to assign\n",
    "empty values in our CSV to NaN `keep_default_na=False, na_values=[\"\"]`.\n",
    "[http://pandas.pydata.org/pandas-docs/dev/generated/pandas.io.parsers.read_csv.html](More\n",
    "about all of the read_csv options here.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatinating\n",
    "\n",
    "We can use the `concat` function in Pandas to append either columns or rows from\n",
    "one DataFrame to another.  Let's grab two subsets of our data to see how this\n",
    "works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in first 10 lines of gal_df table\n",
    "gal_sub=gal_df.head(10)\n",
    "gal_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grab the last 10 rows (minus the last one)\n",
    "gal_last10=gal_df[-11:-1]\n",
    "gal_last10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reset the index values to the second dataframe appends properly\n",
    "# drop=True option avoids adding new index column with old index values\n",
    "gal_last10 = gal_last10.reset_index(drop=True)\n",
    "gal_last10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we concatenate DataFrames, we need to specify the axis. `axis=0` tells\n",
    "Pandas to stack the second DataFrame under the first one. It will automatically\n",
    "detect whether the column names are the same and will stack accordingly.\n",
    "`axis=1` will stack the columns in the second DataFrame to the RIGHT of the\n",
    "first DataFrame. To stack the data vertically, we need to make sure we have the\n",
    "same columns and associated column format in both datasets. When we stack\n",
    "horizonally, we want to make sure what we are doing makes sense (ie the data are\n",
    "related in some way).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stack the DataFrames on top of each other\n",
    "vertical_stack = pd.concat([gal_sub,gal_last10], axis=0)\n",
    "vertical_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# place the DataFrames side by side\n",
    "horizontal_stack = pd.concat([gal_sub,gal_last10], axis=1)\n",
    "horizontal_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge \n",
    "**Row Index Values and Concat**\n",
    "\n",
    "Have a look at the `vertical_stack` dataframe? Notice anything unusual?\n",
    "The row indexes for the two data frames `survey_sub` and `survey_sub_last10`\n",
    "have been repeated. We can reindex the new dataframe using the `reset_index()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining DataFrames\n",
    "\n",
    "When we concatenated our DataFrames we simply added them to each other -\n",
    "stacking them either vertically or side by side. Another way to combine\n",
    "DataFrames is to use columns in each dataset that contain common values (a\n",
    "common unique id). Combining DataFrames using a common field is called\n",
    "\"joining\". The columns containing the common values are called \"join key(s)\".\n",
    "Joining DataFrames in this way is often useful when one DataFrame is a \"lookup\n",
    "table\" containing additional data that we want to include in the other. \n",
    "\n",
    "For example, the `SDSS_blue_centre_query.csv` file that we've been loaded is a query from the SDSS DR7 database for 1000 galaxies with blue centres (things we would expect to be Spirals). \n",
    "This table contains the ID, postition, redshift and r-band magnitude as well as size measurements.\n",
    "\n",
    "## Joining Two DataFrames \n",
    "\n",
    "### Identifying join keys\n",
    "\n",
    "To identify appropriate join keys we first need to know which field(s) are\n",
    "shared between the files (DataFrames). We might inspect both DataFrames to\n",
    "identify these columns. If we are lucky, both DataFrames will have columns with\n",
    "the same name that also contain the same data. If we are less lucky, we need to\n",
    "identify a (differently-named) column in each DataFrame that contains the same\n",
    "information.\n",
    "\n",
    "Check the column names for gal_df and sdss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(gal_df.columns.values)\n",
    "print(sdss.columns.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, the join key is the column containing the object\n",
    "identifier. We could also use 'ra' and 'dec', however, they are not presented in the same units thus joining on id will be esier.\n",
    "\n",
    "There are [different types of joins](http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/), so we\n",
    "also need to decide which type of join makes sense for our analysis.\n",
    "\n",
    "## Inner joins\n",
    "\n",
    "The most common type of join is called an _inner join_. An inner join combines\n",
    "two DataFrames based on a join key and returns a new DataFrame that contains\n",
    "**only** those rows that have matching values in *both* of the original\n",
    "DataFrames. \n",
    "\n",
    "Inner joins yield a DataFrame that contains only rows where the value being\n",
    "joins exists in BOTH tables. An example of an inner join, adapted from [this\n",
    "page](http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/) is below:\n",
    "\n",
    "![Inner join -- courtesy of codinghorror.com](http://blog.codinghorror.com/content/images/uploads/2007/10/6a0120a85dcdae970b012877702708970c-pi.png)\n",
    "\n",
    "The pandas function for performing joins is called `merge` and an inner join is\n",
    "the default option:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_inner = pd.merge(left=gal_df,right=sdss, left_on='id', right_on='objID')\n",
    "# if both column names were 'id', we could skip the `left_on`\n",
    "# and `right_on` arguments and still get the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# what's the size of the output data?\n",
    "merged_inner.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of an inner join of `gal_df` and `sdss` is a new DataFrame\n",
    "that contains the combined set of columns from `gal_df` and `sdss`. It\n",
    "*only* contains rows that have an ID that is the same in\n",
    "both the `gal_df` and `sdss` DataFrames. \n",
    "\n",
    "In other words, if a row in\n",
    "`gal_df` has a value of `id` that does *not* appear in the `objID`\n",
    "column of `sdss`, it will not be included in the DataFrame returned by an\n",
    "inner join.  Similarly, if a row in `sdss` has a value of `objID`\n",
    "that does *not* appear in the `id` column of `gal_df`, that row will not\n",
    "be included in the DataFrame returned by an inner join.\n",
    "\n",
    "The two DataFrames that we want to join are passed to the `merge` function using\n",
    "the `left` and `right` argument. The `left_on='id'` argument tells `merge`\n",
    "to use the `id` column as the join key from `gal_df` (the `left`\n",
    "DataFrame). Similarly , the `right_on='objID'` argument tells `merge` to\n",
    "use the `objID` column as the join key from `sdss` (the `right`\n",
    "DataFrame). For inner joins, the order of the `left` and `right` arguments does\n",
    "not matter.\n",
    "\n",
    "The result `merged_inner` DataFrame contains all of the columns from `survey_sub` as well as all the columns from `sdss`.\n",
    "\n",
    "Notice that `merged_inner` has way fewer rows than `gal_df`. This is an\n",
    "indication that there were rows in `gal_df` with value(s) for `id` that\n",
    "do not exist as value(s) for `objID` in `sdss`.\n",
    " \n",
    "## Left joins\n",
    "\n",
    "What if we want to add information from `sdss` to `gal_df` without\n",
    "losing any of the information from `gal_df`? In this case, we use a different\n",
    "type of join called a \"left outer join\", or a \"left join\".\n",
    "\n",
    "Like an inner join, a left join uses join keys to combine two DataFrames. Unlike\n",
    "an inner join, a left join will return *all* of the rows from the `left`\n",
    "DataFrame, even those rows whose join key(s) do not have values in the `right`\n",
    "DataFrame.  Rows in the `left` DataFrame that are missing values for the join\n",
    "key(s) in the `right` DataFrame will simply have null (i.e., NaN or None) values\n",
    "for those columns in the resulting joined DataFrame.\n",
    "\n",
    "Note: a left join will still discard rows from the `right` DataFrame that do not\n",
    "have values for the join key(s) in the `left` DataFrame.\n",
    "\n",
    "![Left Join](http://blog.codinghorror.com/content/images/uploads/2007/10/6a0120a85dcdae970b01287770273e970c-pi.png)\n",
    "\n",
    "A left join is performed in pandas by calling the same `merge` function used for\n",
    "inner join, but using the `how='left'` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_left = pd.merge(left=gal_df,right=sdss, left_on='id', right_on='objID', how='left')\n",
    "\n",
    "merged_left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result DataFrame from a left join (`merged_left`) looks very much like the\n",
    "result DataFrame from an inner join (`merged_inner`) in terms of the columns it\n",
    "contains. However, unlike `merged_inner`, `merged_left` contains the **same\n",
    "number of rows** as the original `gal_df` DataFrame. When we inspect\n",
    "`merged_left`, we find there are rows where the information that should have\n",
    "come from `sdss` (e.g., `z`, `r`, and `expRad_r`) is\n",
    "missing (they contain NaN values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(merged_left[ pd.isnull(merged_left.z) ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These rows are the ones where the value of `id` from `sdss` does not occur in `gal_df`.\n",
    "\n",
    "\n",
    "## Other join types\n",
    "\n",
    "The pandas `merge` function supports two other join types:\n",
    "\n",
    "* Right (outer) join: Invoked by passing `how='right'` as an argument. Similar\n",
    "  to a left join, except *all* rows from the `right` DataFrame are kept, while\n",
    "  rows from the `left` DataFrame without matching join key(s) values are\n",
    "  discarded.\n",
    "* Full (outer) join: Invoked by passing `how='outer'` as an argument. This join\n",
    "  type returns the all pairwise combinations of rows from both DataFrames; i.e.,\n",
    "  the result DataFrame will `NaN` where data is missing in one of the dataframes. This join type is\n",
    "  very rarely used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
